rdd,use memory or ExternalBlockStore whether to drop the RDD to disk if it falls out of memory or ExternalBlockStore whether to keep the data in memory in a serialized format and whether to replicate the RDD partitions on multiple nodes
spark,it ships a copy of each variable used in the function to each task Sometimes a variable needs to be shared across tasks or between tasks and the driver program Spark supports two types of shared variables broadcast variables which can be used to cache a value in memory on all nodes and accumulators which are variables that are only “added” to such as counters and sums
kafka,Kafka Streams creates a state store to perform the aggregation here called metrics-agg-store and this state store is backed by a changelog effectively another internal topic to make it fault-tolerant The changelog topic basically keeps track of the updates made to the state store